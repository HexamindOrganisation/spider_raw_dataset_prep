{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the dataset into a csv file from the json file\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def extract_data(json_file, csv_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    with open(csv_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data[0].keys())\n",
    "        for row in data:\n",
    "            writer.writerow(row.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = './spider/train_spider.json'\n",
    "csv_file = './spider/train_spider.csv'\n",
    "extract_data(json_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = './spider/dev.json'\n",
    "csv_file = './spider/dev_spider.csv'\n",
    "extract_data(json_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link the csv file to the database in ./spider/database\n",
    "\n",
    "def dbid_to_schema(db_id):\n",
    "    with open('./spider/database/' + db_id + '/schema.sql') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all db_ids in the ./spider/database folder\n",
    "import os\n",
    "db_ids_train = os.listdir('./spider/database')\n",
    "db_ids_train = [db_id for db_id in db_ids_train if os.path.isdir('./spider/database/' + db_id)]\n",
    "\n",
    "# remove from list all db_ids who do not have a schema.sql file, and put them in a new list\n",
    "db_ids_no_schema = []\n",
    "for db_id in db_ids_train:\n",
    "    try:\n",
    "        dbid_to_schema(db_id)\n",
    "    except:\n",
    "        db_ids_no_schema.append(db_id)\n",
    "db_ids_train = [db_id for db_id in db_ids_train if db_id not in db_ids_no_schema]\n",
    "\n",
    "# remove from the no_schema list all db_ids who do not have a .sql file, and put them in a new list\n",
    "db_ids_no_sql = ['chinook_1', 'company_1', 'epinions_1', 'flight_4', 'icfp_1', 'small_bank_1', 'twitter_1', 'voter_1', 'world_1']\n",
    "db_ids_no_schema = [db_id for db_id in db_ids_no_schema if db_id not in db_ids_no_sql]\n",
    "\n",
    "# add outliers:\n",
    "db_ids_outliers = ['college_1', 'college_2']\n",
    "schema_outliers = ['TinyCollege.sql', 'TextBookExampleSchema.sql']\n",
    "\n",
    "# remove all outliers from db_ids_no_schema\n",
    "db_ids_no_schema = [db_id for db_id in db_ids_no_schema if db_id not in db_ids_outliers]\n",
    "\n",
    "# remove the db_ids_no_sql from the train_spider.csv file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./spider/train_spider.csv')\n",
    "df = df[~df['db_id'].isin(db_ids_no_sql)]\n",
    "df = df[~df['db_id'].isin(db_ids_no_schema)]\n",
    "df = df[~df['db_id'].isin(db_ids_outliers)]\n",
    "df.to_csv('./spider/train_spider.csv', index=False)\n",
    "\n",
    "# remove the db_ids_no_sql from the dev_spider.csv file\n",
    "df = pd.read_csv('./spider/dev_spider.csv')\n",
    "df = df[~df['db_id'].isin(db_ids_no_sql)]\n",
    "df = df[~df['db_id'].isin(db_ids_no_schema)]\n",
    "df = df[~df['db_id'].isin(db_ids_outliers)]\n",
    "df.to_csv('./spider/dev_spider.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# create one csv file with two columns: db_id and schema\n",
    "csv_file = './spider/database_schema.csv'\n",
    "\n",
    "with open(csv_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['db_id', 'schema'])\n",
    "    for db_id in db_ids_no_schema:\n",
    "        with open('./spider/database/' + db_id + '/' + db_id + '.sql') as f:\n",
    "            schema = f.readlines()\n",
    "\n",
    "            # remove all INSERT lines from the schema except one per table\n",
    "            kept_inserts = []\n",
    "            used_tables = []\n",
    "            for line in schema:\n",
    "                if line.startswith('INSERT') or line.startswith('insert') or line.startswith(' insert') or line.startswith(' INSERT') or line.startswith('Insert') or line.startswith(' Insert') or line.startswith('\\tINSERT') or line.startswith('\\tinsert') or line.startswith('\\tInsert'):\n",
    "                    line_tokens = line.split()\n",
    "                    table_name = line_tokens[2]\n",
    "                    # remove all the line after the fourth occurence of ')'\n",
    "                    if line.count(')') > 4:\n",
    "                        first_find = line.find(')')\n",
    "                        second_find = line.find(')', first_find + 1)\n",
    "                        third_find = line.find(')', second_find + 1)\n",
    "                        fourth_find = line.find(')', third_find + 1)\n",
    "                        line = line[:line.find(')', fourth_find + 1)] + ')'             \n",
    "                    if table_name in used_tables:\n",
    "                        continue\n",
    "                    else:\n",
    "                        used_tables.append(table_name)\n",
    "                        kept_inserts.append(line)\n",
    "            schema = [line for line in schema if not line.startswith('INSERT') and not line.startswith('insert') \n",
    "                      and not line.startswith(' insert') and not line.startswith(' INSERT')\n",
    "                      and not line.startswith('Insert') and not line.startswith(' Insert')\n",
    "                      and not line.startswith('\\tINSERT') and not line.startswith('\\tinsert') and not line.startswith('\\tInsert')]\n",
    "            schema = schema + kept_inserts\n",
    "            schema = ''.join(schema)\n",
    "        writer.writerow([db_id, schema])\n",
    "    for db_id in db_ids_train:\n",
    "        schema = dbid_to_schema(db_id)\n",
    "        # remove all INSERT lines from the schema except one per table\n",
    "        kept_inserts = []\n",
    "        used_tables = []\n",
    "        for line in schema:\n",
    "            if line.startswith('INSERT') or line.startswith('insert') or line.startswith(' insert') or line.startswith(' INSERT') or line.startswith('Insert') or line.startswith(' Insert') or line.startswith('\\tINSERT') or line.startswith('\\tinsert') or line.startswith('\\tInsert'):\n",
    "                line_tokens = line.split()\n",
    "                table_name = line_tokens[2]\n",
    "                # remove all the line after the fourth occurence of ')'\n",
    "                if line.count(')') > 4:\n",
    "                    first_find = line.find(')')\n",
    "                    second_find = line.find(')', first_find + 1)\n",
    "                    third_find = line.find(')', second_find + 1)\n",
    "                    fourth_find = line.find(')', third_find + 1)\n",
    "                    line = line[:line.find(')', fourth_find + 1)] + ')'                 \n",
    "                if table_name in used_tables:\n",
    "                    continue\n",
    "                else:\n",
    "                    used_tables.append(table_name)\n",
    "                    kept_inserts.append(line)\n",
    "        schema = [line for line in schema if not line.startswith('INSERT') and not line.startswith('insert') \n",
    "                    and not line.startswith(' insert') and not line.startswith(' INSERT')\n",
    "                    and not line.startswith('Insert') and not line.startswith(' Insert')\n",
    "                    and not line.startswith('\\tINSERT') and not line.startswith('\\tinsert') and not line.startswith('\\tInsert')]\n",
    "        schema = schema + kept_inserts\n",
    "        schema = ''.join(schema)\n",
    "        writer.writerow([db_id, schema])\n",
    "    for db_id in db_ids_outliers:\n",
    "        with open('./spider/database/' + db_id + '/' + schema_outliers[db_ids_outliers.index(db_id)]) as f:\n",
    "            schema = f.readlines()\n",
    "\n",
    "            # remove all INSERT lines from the schema except one per table\n",
    "            kept_inserts = []\n",
    "            used_tables = []\n",
    "            for line in schema:\n",
    "                if line.startswith('INSERT') or line.startswith('insert') or line.startswith(' insert') or line.startswith(' INSERT') or line.startswith('Insert') or line.startswith(' Insert') or line.startswith('\\tINSERT') or line.startswith('\\tinsert') or line.startswith('\\tInsert'):\n",
    "                    line_tokens = line.split()\n",
    "                    table_name = line_tokens[2]\n",
    "                    # remove all the line after the fourth occurence of ')'\n",
    "                    if line.count(')') > 4:\n",
    "                        first_find = line.find(')')\n",
    "                        second_find = line.find(')', first_find + 1)\n",
    "                        third_find = line.find(')', second_find + 1)\n",
    "                        fourth_find = line.find(')', third_find + 1)\n",
    "                        line = line[:line.find(')', fourth_find + 1)] + ')'                \n",
    "                    if table_name in used_tables:\n",
    "                        continue\n",
    "                    else:\n",
    "                        used_tables.append(table_name)\n",
    "                        kept_inserts.append(line)\n",
    "            schema = [line for line in schema if not line.startswith('INSERT') and not line.startswith('insert') \n",
    "                      and not line.startswith(' insert') and not line.startswith(' INSERT')\n",
    "                      and not line.startswith('Insert') and not line.startswith(' Insert')\n",
    "                      and not line.startswith('\\tINSERT') and not line.startswith('\\tinsert') and not line.startswith('\\tInsert')]\n",
    "            schema = schema + kept_inserts\n",
    "            schema = ''.join(schema)\n",
    "        writer.writerow([db_id, schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add insert statements to schema when it is missing\n",
    "\n",
    "import pandas as pd\n",
    "db_ids_not_insert = []\n",
    "df = pd.read_csv('./spider/database_schema.csv')\n",
    "db_ids = df['db_id'].tolist()\n",
    "for db_id in db_ids:\n",
    "    schema = df[df['db_id'] == db_id]['schema'].values[0]\n",
    "    if ('INSERT' or 'insert' or 'Insert') not in schema:\n",
    "        db_ids_not_insert.append(db_id)\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def get_values(db_id):\n",
    "    conn = sqlite3.connect(f'./spider/database/{db_id}/{db_id}.sqlite')\n",
    "    conn.text_factory = lambda b: b.decode(errors = 'ignore')\n",
    "    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "    values = []\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        value = conn.execute(f\"SELECT * FROM {table_name}\").fetchall()[:2]\n",
    "        values.append(value)\n",
    "    conn.close()\n",
    "    return tables, values\n",
    "\n",
    "for db_id in db_ids_not_insert:\n",
    "    tables, values = get_values(db_id)\n",
    "    inserts = []\n",
    "    for i, table in enumerate(tables):\n",
    "        table_name = table[0]\n",
    "        if len(values[i]) != 0:\n",
    "            insert = f'INSERT INTO {table_name} VALUES {values[i]}'\n",
    "            inserts.append(insert)\n",
    "    inserts = '\\n'.join(inserts)\n",
    "    df.loc[df['db_id'] == db_id, 'schema'] = df.loc[df['db_id'] == db_id, 'schema'] + inserts\n",
    "df.to_csv('./spider/database_schema.csv', index=False)\n",
    "\n",
    "# Note: only 7 databases have no insert statements in the schema, but they don't have any data in the tables either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# check if there are db_ids in the dev_spider.csv that are not in the database_schema.csv\n",
    "df_dev = pd.read_csv('./spider/dev_spider.csv')\n",
    "df_schema = pd.read_csv('./spider/database_schema.csv')\n",
    "\n",
    "db_ids_dev = df_dev['db_id'].unique()\n",
    "db_ids_schema = df_schema['db_id'].unique()\n",
    "\n",
    "db_ids_no_schema_dev = [db_id for db_id in db_ids_dev if db_id not in db_ids_schema]\n",
    "print(db_ids_no_schema_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace insert with \\nINSERT\n",
    "df_schema = pd.read_csv('./spider/database_schema.csv')\n",
    "df_test = pd.read_csv('./spider/test_database_schema.csv')\n",
    "\n",
    "df_schema['schema'] = df_schema['schema'].str.replace('INSERT', '\\nINSERT')\n",
    "df_schema['schema'] = df_schema['schema'].str.replace('insert', '\\nINSERT')\n",
    "df_schema['schema'] = df_schema['schema'].str.replace('Insert', '\\nINSERT')\n",
    "df_test['schema'] = df_test['schema'].str.replace('INSERT', '\\nINSERT')\n",
    "df_test['schema'] = df_test['schema'].str.replace('insert', '\\nINSERT')\n",
    "df_test['schema'] = df_test['schema'].str.replace('Insert', '\\nINSERT')\n",
    "\n",
    "df_schema.to_csv('./spider/database_schema.csv', index=False)\n",
    "df_test.to_csv('./spider/test_database_schema.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from database_schema.csv, create a clean_database_schema.csv file with the same schema as the database_schema.csv file,\n",
    "# but that removes lines that are not part of a CREATE TABLE statement, or an INSERT INTO statement\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file = './spider/clean_database_schema.csv'\n",
    "\n",
    "glossary_create = ['CREATE', 'create', 'Create']\n",
    "glossary_insert = ['INSERT', 'insert', 'Insert']\n",
    "glossary_table = ['TABLE ', 'table ', 'Table ']\n",
    "\n",
    "with open('./spider/database_schema.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    with open(csv_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['db_id', 'schema'])\n",
    "        for row in reader:\n",
    "            db_id = row[0]\n",
    "            schema = row[1]\n",
    "            # remove all characters in schema before a create or CREATE or Create\n",
    "            lower_schema = schema.lower()\n",
    "            create_index = lower_schema.find('create')\n",
    "            if create_index == -1:\n",
    "                continue\n",
    "            schema = schema[create_index:]\n",
    "\n",
    "            lines = schema.split('\\n')\n",
    "            new_lines = []\n",
    "            inside_create = False\n",
    "            for line in lines:\n",
    "                if any(word in line for word in glossary_create):\n",
    "                    if any(word in line for word in glossary_table):\n",
    "                        new_lines.append(line)\n",
    "                        inside_create = True\n",
    "                elif any(word in line for word in glossary_insert):\n",
    "                    new_lines.append(line)\n",
    "                elif inside_create:\n",
    "                    new_lines.append(line)\n",
    "                if ';' in line:\n",
    "                    inside_create = False\n",
    "            schema = '\\n'.join(new_lines)\n",
    "            writer.writerow([db_id, schema])\n",
    "\n",
    "csv_file = './spider/clean_test_database_schema.csv'\n",
    "\n",
    "with open('./spider/test_database_schema.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    with open(csv_file, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['db_id', 'schema'])\n",
    "        for row in reader:\n",
    "            db_id = row[0]\n",
    "            schema = row[1]\n",
    "            # remove all characters in schema before a create or CREATE or Create\n",
    "            lower_schema = schema.lower()\n",
    "            create_index = lower_schema.find('create')\n",
    "            if create_index == -1:\n",
    "                continue\n",
    "            schema = schema[create_index:]\n",
    "\n",
    "            lines = schema.split('\\n')\n",
    "            new_lines = []\n",
    "            inside_create = False\n",
    "            for line in lines:\n",
    "                if any(word in line for word in glossary_create):\n",
    "                    if any(word in line for word in glossary_table):\n",
    "                        new_lines.append(line)\n",
    "                        inside_create = True\n",
    "                elif any(word in line for word in glossary_insert):\n",
    "                    new_lines.append(line)\n",
    "                elif inside_create:\n",
    "                    new_lines.append(line)\n",
    "                if ';' in line:\n",
    "                    inside_create = False\n",
    "            schema = '\\n'.join(new_lines)\n",
    "            writer.writerow([db_id, schema])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_schema(sample):\n",
    "    db_id = sample['db_id']\n",
    "    with open('./spider/database/' + db_id + '/schema.sql') as f:\n",
    "        lines = f.readlines()\n",
    "    new_sample = {}\n",
    "    # concatenate the schema to a single string\n",
    "    lines = ' '.join(lines)\n",
    "    new_sample['schema'] = lines\n",
    "    new_sample['query'] = sample['query']\n",
    "    new_sample['question'] = sample['question']\n",
    "    new_sample['db_id'] = db_id\n",
    "    return new_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one sample from the csv file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./spider/train_spider.csv\")\n",
    "sample = df.iloc[0].to_dict()\n",
    "\n",
    "sample = sample_schema(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new csv files for the train and dev datasets with only db_id, query and question, but without the schema\n",
    "def clean_csv(csv_file, new_csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    new_df = pd.DataFrame(columns=['db_id', 'query', 'question'])\n",
    "    for i in range(len(df)):\n",
    "        sample = df.iloc[i].to_dict()\n",
    "        sample = sample['db_id'], sample['query'], sample['question']\n",
    "        new_df.loc[i] = sample\n",
    "    new_df.to_csv(new_csv_file, index=False)\n",
    "\n",
    "clean_csv('./spider/train_spider.csv', './spider/train_spider_clean.csv')\n",
    "clean_csv('./spider/dev_spider.csv', './spider/dev_spider_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a4781482e14b03b631b8b86eb81e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c846715e1e48609fa22f0a97a5bb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/665 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecedca8caec644e4b3ffbe106d753b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a Huggingface dataset from the csv files\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('./spider/train_spider_clean.csv')\n",
    "df_dev = pd.read_csv('./spider/dev_spider_clean.csv')\n",
    "df_test = pd.read_csv('./spider/test_spider_clean.csv')\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "dev_dataset = Dataset.from_pandas(df_dev)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "df_schema = pd.read_csv('./spider/clean_database_schema.csv')\n",
    "df_schema_test = pd.read_csv('./spider/clean_test_database_schema.csv')\n",
    "\n",
    "# replace the db_id with the schema in the ./spider/database_schema.csv file, and join the schema into a single string\n",
    "def replace_db_id_with_schema(sample):\n",
    "    db_id = sample['db_id']\n",
    "    schema = df_schema[df_schema['db_id'] == db_id]['schema'].values[0]\n",
    "    sample['schema'] = schema\n",
    "    # del sample['db_id']\n",
    "    return sample\n",
    "\n",
    "def replace_db_id_with_schema_test(sample):\n",
    "    db_id = sample['db_id']\n",
    "    schema = df_schema_test[df_schema_test['db_id'] == db_id]['schema'].values[0]\n",
    "    sample['schema'] = schema\n",
    "    # del sample['db_id']\n",
    "    return sample\n",
    "\n",
    "train_dataset = train_dataset.map(replace_db_id_with_schema)\n",
    "dev_dataset = dev_dataset.map(replace_db_id_with_schema)\n",
    "test_dataset = test_dataset.map(replace_db_id_with_schema_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a datasetdict and push to the Huggingface Hub on my account to VictorDCh/spider-clean-text-to-sql (the repo already exists)\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset_dict = DatasetDict({'train': train_dataset, 'dev': dev_dataset, 'test': test_dataset})\n",
    "# dataset_dict.save_to_disk('./spider_dataset') # already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\vdubu\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122f0c999ba147fc953634fda8d1d4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9d2f0531d64a5087487e2aae338aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48796d00b23426a93bfd38482e234fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3947b491ada04630a46fcf29bafa00f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dcb040e06441f19fcdaa65406fce09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dc04aaa04d49ac9831bee0f143fef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/VictorDCh/spider-clean-text-to-sql-5/commit/731fc74c2e2e586de05ce4ac8fb07e743d90bd07', commit_message='Upload dataset', commit_description='', oid='731fc74c2e2e586de05ce4ac8fb07e743d90bd07', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# write token for huggingface-cli\n",
    "print(\"Need to write token for huggingface-cli!\")\n",
    "#!huggingface-cli login --token [insert token with write permission here here]\n",
    "\n",
    "# dataset_dict = DatasetDict.load_from_disk('./spider_dataset')\n",
    "dataset_dict.push_to_hub('VictorDCh/spider-clean-text-to-sql-5', private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c997f35b15a4ff8b66052a361e00934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/591 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a420b845694e1487f504f6480d088e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/450k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2366feca4743ddacfaa6b70334dafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51916eee03b54616b44d7102473d1931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/126k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2781ae1ec8046b9947d48f3daf365bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbddba106d194e4a93f9a9d3babe56be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/665 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8de7b897ba4e8db453700391f1f464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dshf = load_dataset('VictorDCh/spider-clean-text-to-sql-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'department_management', 'query': 'SELECT count(*) FROM head WHERE age  >  56', 'question': 'How many heads of the departments are older than 56 ?', 'schema': 'CREATE TABLE IF NOT EXISTS \"department\" (\\r\\n\"Department_ID\" int,\\r\\n\"Name\" text,\\r\\n\"Creation\" text,\\r\\n\"Ranking\" int,\\r\\n\"Budget_in_Billions\" real,\\r\\n\"Num_Employees\" real,\\r\\nPRIMARY KEY (\"Department_ID\")\\r\\n);\\r\\nCREATE TABLE IF NOT EXISTS \"head\" (\\r\\n\"head_ID\" int,\\r\\n\"name\" text,\\r\\n\"born_state\" text,\\r\\n\"age\" real,\\r\\nPRIMARY KEY (\"head_ID\")\\r\\n);\\r\\nCREATE TABLE IF NOT EXISTS \"management\" (\\r\\n\"department_ID\" int,\\r\\n\"head_ID\" int,\\r\\n\"temporary_acting\" text,\\r\\nPRIMARY KEY (\"Department_ID\",\"head_ID\"),\\r\\nFOREIGN KEY (\"Department_ID\") REFERENCES `department`(\"Department_ID\"),\\r\\nFOREIGN KEY (\"head_ID\") REFERENCES `head`(\"head_ID\")\\r\\n);\\r\\nINSERT INTO department VALUES(1,\\'State\\',\\'1789\\',\\'1\\',9.9600000000000008526,30265.999999999999999);\\r\\nINSERT INTO head VALUES(1,\\'Tiger Woods\\',\\'Alabama\\',66.999999999999999998);\\r\\nINSERT INTO management VALUES(2,5,\\'Yes\\');'}\n"
     ]
    }
   ],
   "source": [
    "# get sample\n",
    "sample = dshf['train'][0]\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS \"department\" (\n",
      "\"Department_ID\" int,\n",
      "\"Name\" text,\n",
      "\"Creation\" text,\n",
      "\"Ranking\" int,\n",
      "\"Budget_in_Billions\" real,\n",
      "\"Num_Employees\" real,\n",
      "PRIMARY KEY (\"Department_ID\")\n",
      ");\n",
      "CREATE TABLE IF NOT EXISTS \"head\" (\n",
      "\"head_ID\" int,\n",
      "\"name\" text,\n",
      "\"born_state\" text,\n",
      "\"age\" real,\n",
      "PRIMARY KEY (\"head_ID\")\n",
      ");\n",
      "CREATE TABLE IF NOT EXISTS \"management\" (\n",
      "\"department_ID\" int,\n",
      "\"head_ID\" int,\n",
      "\"temporary_acting\" text,\n",
      "PRIMARY KEY (\"Department_ID\",\"head_ID\"),\n",
      "FOREIGN KEY (\"Department_ID\") REFERENCES `department`(\"Department_ID\"),\n",
      "FOREIGN KEY (\"head_ID\") REFERENCES `head`(\"head_ID\")\n",
      ");\n",
      "INSERT INTO department VALUES(1,'State','1789','1',9.9600000000000008526,30265.999999999999999);\n",
      "INSERT INTO head VALUES(1,'Tiger Woods','Alabama',66.999999999999999998);\n",
      "INSERT INTO management VALUES(2,5,'Yes');\n"
     ]
    }
   ],
   "source": [
    "print(dshf[\"train\"][0][\"schema\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
